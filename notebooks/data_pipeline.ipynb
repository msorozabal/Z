{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9310a0",
   "metadata": {},
   "source": [
    "### La tarea debe modelar una tubería (pipeline) de datos que ejecuta los siguientes pasos\n",
    "##### Paso 1 (ejecución en paralelo)\n",
    "- Verificar el esquema de datos;\n",
    "- Verificar datos perdidos/faltantes;\n",
    "- Verificar datos erróneos/anómalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24545f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import mean as mean_func\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DataPipeline\").getOrCreate()\n",
    "\n",
    "# Cargar los datos en un DataFrame\n",
    "df = spark.read.csv(\"backend-dev-data-dataset.txt\", header=True, inferSchema=True)\n",
    "\n",
    "#### PASO 1: \n",
    "\n",
    "# Función para verificar el esquema de datos\n",
    "def check_schema(df):\n",
    "    print(\"Data schema:\")\n",
    "    df.printSchema()    \n",
    "\n",
    "# Verificar datos perdidos/faltantes \n",
    "def missing_data(df):\n",
    "    for c in df.columns:\n",
    "        df = df.withColumn(c, \\\n",
    "                      when(col(c)==\"na\", None) \\\n",
    "                      .otherwise(col(c))) \n",
    "\n",
    "    missing_data_count = [df.filter(col(c).isNull()).count() for c in df.columns]\n",
    "    print(\"Number of rows with missing data per column: \", missing_data_count)\n",
    "    \n",
    "    # Acá me quedo duda con la letra, ya que dice \"VERIFICAR\" no pide corregirlo.\n",
    "    # Si hubiera que corregirlo utilizaría una estrategia de Mediana o usar percentil 50 y retornaría el df.\n",
    "    \n",
    "# Verificar datos erróneos/anómalos\n",
    "def find_outliers(df, interval):\n",
    "    for c in df.columns:\n",
    "        col_type = df.schema[c].dataType\n",
    "\n",
    "        if col_type == DoubleType():   \n",
    "            mean = df.select(mean_func(col(c))).first()[0]\n",
    "            std = df.select(stddev(col(c))).first()[0]\n",
    "            \n",
    "            # Se propone un estudio de outliers, pueden hacerse otros como box-plots, etc. \n",
    "            # Filtro las filas que tengan valor fuera de 3 standard deviations de la media\n",
    "            outliers = df.filter((col(c) < (mean - interval * std)) | (col(c) > (mean + interval * std)))\n",
    "            print(f'Total outliers for column {c} : {outliers.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb92d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data schema:\n",
      "root\n",
      " |-- key_1: string (nullable = true)\n",
      " |-- date_2: timestamp (nullable = true)\n",
      " |-- cont_3: double (nullable = true)\n",
      " |-- cont_4: double (nullable = true)\n",
      " |-- disc_5: integer (nullable = true)\n",
      " |-- disc_6: string (nullable = true)\n",
      " |-- cat_7: string (nullable = true)\n",
      " |-- cat_8: string (nullable = true)\n",
      " |-- cont_9: double (nullable = true)\n",
      " |-- cont_10: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outliers for column cont_3 : 17977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outliers for column cont_4 : 2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing data per column:  [0, 0, 0, 0, 0, 100327, 0, 0, 0, 100266]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outliers for column cont_9 : 2752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ejecutar las funciones en paralelo\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "\"\"\" concurrent.futures ThreadPoolExecutor nos permite ejecutar funciones en paralelo en distintos hilos o procesos. \"\"\"\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = [executor.submit(check_schema, df), \n",
    "               executor.submit(missing_data, df),\n",
    "               executor.submit(find_outliers, df, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ec8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b81de5e8",
   "metadata": {},
   "source": [
    "#### Paso 2 (ejecución secuencial)\n",
    "- Normalizar una columna (cualquiera de valores continuos);\n",
    "- Filtrar una columna por cierto valor (cualquiera de valores categóricos);\n",
    "- Agrupar ciertas columnas (cualesquiera que correspondan a fechas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba46717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PASO 2: \n",
    "\n",
    "def normalize_column(df, column):\n",
    "    col_mean = df.agg(mean(f\"{column}\")).first()[0]\n",
    "    col_std = df.agg(stddev(f\"{column}\")).first()[0]\n",
    "    scaled_df = df.select(\"*\", ((col(f\"{column}\") - col_mean) / col_std).alias(f\"normalized_{column}\"))\n",
    "    return scaled_df\n",
    "    \n",
    "def filter_column(df, column, filter_value):\n",
    "    df = df.filter(col(column) == filter_value)\n",
    "    return df\n",
    "\n",
    "def grouped_avg_data_monthly(df, group_column, date_column):\n",
    "    grouped_df = df.groupBy(month(col(date_column)).alias(\"month\"))\n",
    "    grouped_df = grouped_df.agg(avg(group_column).alias(f\"avg_{group_column}\"))\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88435ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+--------------------+\n",
      "| key_1|             date_2| cont_3|cont_4|disc_5|disc_6|   cat_7|    cat_8|cont_9|cont_10|   normalized_cont_9|\n",
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+--------------------+\n",
      "|HC2030|2016-11-16 00:00:00| 622.27| -2.36|     2|     6|frequent|    happy|  0.24|   0.25|  0.2397046450812019|\n",
      "|sP8147|2004-02-18 00:00:00|1056.16| 59.93|     2|     8|   never|    happy|  1.94|   2.29|  1.9409134599100886|\n",
      "|Cq3823|2007-03-25 00:00:00| 210.73|-93.94|     1|     1|   never|    happy| -0.11|   -0.1|-0.11054422856003945|\n",
      "|Hw9428|2013-12-28 00:00:00|1116.48| 80.58|     3|    10|   never|surprised|  1.27|   1.15|  1.2704370446539979|\n",
      "|xZ0360|2003-08-25 00:00:00| 1038.3| 12.37|     6|    17|   never|    happy|  1.76|   1.76|  1.7607854677517358|\n",
      "|IK2721|2012-10-19 00:00:00| 835.17|  16.3|     4|    11|frequent|surprised|  2.04|    2.3|   2.040984566664729|\n",
      "|iK8875|2005-02-04 00:00:00| 769.02| 75.69|     3|     2|   never|    happy| -1.53|  -1.56|  -1.531553944475933|\n",
      "|qd0312|2014-11-17 00:00:00| 273.11|  66.2|     1|     8|frequent|surprised|  2.67|   2.95|  2.6714325392189635|\n",
      "|IO1104|2020-11-24 00:00:00|1844.21|-54.11|     1|    11|   never|surprised| -0.42|  -0.43|-0.42076465949942465|\n",
      "|mb3668|2002-02-26 00:00:00|2369.77|165.12|     2|     7|   never|    happy| -1.11|  -1.15| -1.1112552961064435|\n",
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+\n",
      "| key_1|             date_2| cont_3|cont_4|disc_5|disc_6|   cat_7|    cat_8|cont_9|cont_10|\n",
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+\n",
      "|HC2030|2016-11-16 00:00:00| 622.27| -2.36|     2|     6|frequent|    happy|  0.24|   0.25|\n",
      "|IK2721|2012-10-19 00:00:00| 835.17|  16.3|     4|    11|frequent|surprised|  2.04|    2.3|\n",
      "|qd0312|2014-11-17 00:00:00| 273.11|  66.2|     1|     8|frequent|surprised|  2.67|   2.95|\n",
      "|MB6485|2009-08-05 00:00:00|2432.88| -9.38|     1|     9|frequent|    happy| -1.15|     na|\n",
      "|eX8597|2004-06-27 00:00:00|  462.2|-15.62|     6|    12|frequent|    happy|  0.25|   0.24|\n",
      "|JW7796|2013-02-15 00:00:00| 610.23| -49.3|     3|     8|frequent|    happy|  0.76|   0.67|\n",
      "|Rh1612|2005-09-12 00:00:00| 450.99| 63.42|     2|    15|frequent|surprised|  0.75|   0.76|\n",
      "|Dv4078|2004-10-01 00:00:00|1851.71|113.44|     1|     8|frequent|surprised|  0.05|   0.06|\n",
      "|sc4013|2017-08-30 00:00:00| 825.26|-29.39|     5|    11|frequent|    happy|  0.26|   0.22|\n",
      "|os0804|2003-06-30 00:00:00| 105.92|  5.41|     4|    16|frequent|    happy| -0.39|  -0.38|\n",
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|        avg_cont_3|\n",
      "+-----+------------------+\n",
      "|   12|1644.4190835669908|\n",
      "|    1|1653.6455568870767|\n",
      "|    6|1654.4757854043357|\n",
      "|    3|1636.5632992579451|\n",
      "|    5|   1642.2685611918|\n",
      "|    9|1669.9463364770413|\n",
      "|    4|1653.8981577273562|\n",
      "|    8|1648.1477843510443|\n",
      "|    7|1648.7407461333883|\n",
      "|   10| 1652.369164142287|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "scaled_df = normalize_column(df, \"cont_9\")\n",
    "filtered_df = filter_column(df, \"cat_7\", \"frequent\")\n",
    "grouped_avg_monthly_df = grouped_avg_data_monthly(df,\"cont_3\", \"date_2\")\n",
    "\n",
    "\n",
    "scaled_df.show(10)\n",
    "filtered_df.show(10)\n",
    "grouped_avg_monthly_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530938c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bfadd52",
   "metadata": {},
   "source": [
    "#### Paso 3 (ejecución secuencial)\n",
    " - Transformar una variable y agregarla al conjunto de datos. (Aplique la función x^3 + exp(y) sobre cualquier tupla de variables continuas);\n",
    " - Agregación - Conteo de registros únicos (sobre cualquier columna devalores categóricos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df539de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PASO 3: \n",
    "\n",
    "def transformed_column(df,x,y):\n",
    "    transformed_column = pow(df[x], 3) + exp(df[y])\n",
    "    df_transformed = df.withColumn(f\"transformed_value_{x}_{y}\", transformed_column)\n",
    "    return df_transformed\n",
    "\n",
    "def unique(df, column):\n",
    "    unique_count = df.agg(countDistinct(col(f\"{column}\")).alias(f\"unique_{column}_count\"))\n",
    "    return unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46bb6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+-------------------------------+\n",
      "| key_1|             date_2| cont_3|cont_4|disc_5|disc_6|   cat_7|    cat_8|cont_9|cont_10|transformed_value_cont_4_cont_9|\n",
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+-------------------------------+\n",
      "|HC2030|2016-11-16 00:00:00| 622.27| -2.36|     2|     6|frequent|    happy|  0.24|   0.25|            -11.873006849678594|\n",
      "|sP8147|2004-02-18 00:00:00|1056.16| 59.93|     2|     8|   never|    happy|  1.94|   2.29|             215251.84040797062|\n",
      "|Cq3823|2007-03-25 00:00:00| 210.73|-93.94|     1|     1|   never|    happy| -0.11|   -0.1|             -828993.6391498647|\n",
      "|Hw9428|2013-12-28 00:00:00|1116.48| 80.58|     3|    10|   never|surprised|  1.27|   1.15|             523220.49196456233|\n",
      "|xZ0360|2003-08-25 00:00:00| 1038.3| 12.37|     6|    17|   never|    happy|  1.76|   1.76|             1898.6314903944024|\n",
      "|IK2721|2012-10-19 00:00:00| 835.17|  16.3|     4|    11|frequent|surprised|  2.04|    2.3|               4338.43760919888|\n",
      "|iK8875|2005-02-04 00:00:00| 769.02| 75.69|     3|     2|   never|    happy| -1.53|  -1.56|             433626.41754466726|\n",
      "|qd0312|2014-11-17 00:00:00| 273.11|  66.2|     1|     8|frequent|surprised|  2.67|   2.95|             290131.96796919283|\n",
      "|IO1104|2020-11-24 00:00:00|1844.21|-54.11|     1|    11|   never|surprised| -0.42|  -0.43|             -158427.5844841802|\n",
      "|mb3668|2002-02-26 00:00:00|2369.77|165.12|     2|     7|   never|    happy| -1.11|  -1.15|              4501933.459286962|\n",
      "+------+-------------------+-------+------+------+------+--------+---------+------+-------+-------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|unique_cat_8_count|\n",
      "+------------------+\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_transformed = transformed_column(df, \"cont_4\", \"cont_9\")\n",
    "df_unique = unique(df, \"cat_8\")\n",
    "\n",
    "df_transformed.show(10)\n",
    "df_unique.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ba03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50e8542a",
   "metadata": {},
   "source": [
    "#### Si quisieramos conectarnos directo con Kinesis Data Stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Step 1: Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"StreamingInference\").getOrCreate()\n",
    "\n",
    "# Step 2: Read the data in real-time\n",
    "df = spark.readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"kinesis_stream_name\") \\\n",
    "    .option(\"awsAccessKey\", \"your_aws_access_key\") \\\n",
    "    .option(\"awsSecretKey\", \"your_aws_secret_key\") \\\n",
    "    .option(\"region\", \"us-west-2\") \\\n",
    "    .load()\n",
    "\n",
    "# Step 3: Transform the data\n",
    "df_transformed = df.selectExpr(\"cast (data as string) as data\") \\\n",
    "    .groupBy(window(df.timestamp, \"10 minutes\", \"5 minutes\")) \\\n",
    "    .agg(count(\"data\").alias(\"count\"))\n",
    "\n",
    "# Step 4: Store the results\n",
    "query = df_transformed.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/streaming_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/streaming_checkpoints\") \\\n",
    "    .start()\n",
    "\n",
    "# Step 5: Start the streaming process\n",
    "query.awaitTermination()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff0416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c4a04b1",
   "metadata": {},
   "source": [
    "#### Paso 4: \n",
    "\n",
    "###### check model_pipeline.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
