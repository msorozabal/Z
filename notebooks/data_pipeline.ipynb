{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65c71c2",
   "metadata": {},
   "source": [
    "### La tarea debe modelar una tubería (pipeline) de datos que ejecuta los siguientes pasos\n",
    "##### Paso 1 (ejecución en paralelo)\n",
    "- Verificar el esquema de datos;\n",
    "- Verificar datos perdidos/faltantes;\n",
    "- Verificar datos erróneos/anómalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea4d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import mean as mean_func\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DataPipeline\").getOrCreate()\n",
    "\n",
    "# Cargar los datos en un DataFrame\n",
    "df = spark.read.csv(\"backend-dev-data-dataset.txt\", header=True, inferSchema=True)\n",
    "\n",
    "#### PASO 1: \n",
    "\n",
    "# Función para verificar el esquema de datos\n",
    "def check_schema(df):\n",
    "    print(\"Data schema:\")\n",
    "    df.printSchema()    \n",
    "\n",
    "# Verificar datos perdidos/faltantes \n",
    "def missing_data(df):\n",
    "    for c in df.columns:\n",
    "        df = df.withColumn(c, \\\n",
    "                      when(col(c)==\"na\", None) \\\n",
    "                      .otherwise(col(c))) \n",
    "\n",
    "    missing_data_count = [df.filter(col(c).isNull()).count() for c in df.columns]\n",
    "    print(\"Number of rows with missing data per column: \", missing_data_count)\n",
    "    \n",
    "    # Acá me quedo duda con la letra, ya que dice \"VERIFICAR\" no pide corregirlo.\n",
    "    # Si hubiera que corregirlo utilizaría una estrategia de Mediana o usar percentil 50 y retornaría el df.\n",
    "    \n",
    "# Verificar datos erróneos/anómalos\n",
    "def find_outliers(df, interval):\n",
    "    for c in df.columns:\n",
    "        col_type = df.schema[c].dataType\n",
    "\n",
    "        if col_type == DoubleType():   \n",
    "            mean = df.select(mean_func(col(c))).first()[0]\n",
    "            std = df.select(stddev(col(c))).first()[0]\n",
    "            \n",
    "            # Se propone un estudio de outliers, pueden hacerse otros como box-plots, etc. \n",
    "            # Filtro las filas que tengan valor fuera de 3 standard deviations de la media\n",
    "            outliers = df.filter((col(c) < (mean - interval * std)) | (col(c) > (mean + interval * std)))\n",
    "            print(f'Total outliers for column {c} : {outliers.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96927167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar las funciones en paralelo\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "\"\"\" concurrent.futures ThreadPoolExecutor nos permite ejecutar funciones en paralelo en distintos hilos o procesos. \"\"\"\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = [executor.submit(check_schema, df), \n",
    "               executor.submit(missing_data, df),\n",
    "               executor.submit(find_outliers, df, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c84b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd8ed582",
   "metadata": {},
   "source": [
    "#### Paso 2 (ejecución secuencial)\n",
    "- Normalizar una columna (cualquiera de valores continuos);\n",
    "- Filtrar una columna por cierto valor (cualquiera de valores categóricos);\n",
    "- Agrupar ciertas columnas (cualesquiera que correspondan a fechas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ea8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PASO 2: \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def normalize_column(df, column):\n",
    "    col_mean = df.agg(mean(f\"{column}\")).first()[0]\n",
    "    col_std = df.agg(stddev(f\"{column}\")).first()[0]\n",
    "    scaled_df = df.select(\"*\", ((col(f\"{column}\") - col_mean) / col_std).alias(f\"normalized_{column}\"))\n",
    "    return scaled_df\n",
    "    \n",
    "def filter_column(df, column, filter_value):\n",
    "    df = df.filter(col(column) == filter_value)\n",
    "    return df\n",
    "\n",
    "def grouped_avg_data_monthly(df, group_column, date_column):\n",
    "    grouped_df = df.groupBy(month(col(date_column)).alias(\"month\"))\n",
    "    grouped_df = grouped_df.agg(avg(group_column).alias(f\"avg_{group_column}\"))\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b770267",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = normalize_column(df, \"cont_9\")\n",
    "filtered_df = filter_column(df, \"cat_7\", \"frequent\")\n",
    "grouped_avg_monthly_df = grouped_avg_data_monthly(df,\"cont_3\", \"date_2\")\n",
    "\n",
    "\n",
    "filtered_df.show(10)\n",
    "grouped_avg_monthly_df.show(10)\n",
    "scaled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339da15c",
   "metadata": {},
   "source": [
    "#### Paso 3 (ejecución secuencial)\n",
    " - Transformar una variable y agregarla al conjunto de datos. (Aplique la función x^3 + exp(y) sobre cualquier tupla de variables continuas);\n",
    " - Agregación - Conteo de registros únicos (sobre cualquier columna devalores categóricos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PASO 3: \n",
    "\n",
    "def transformed_column(df,x,y):\n",
    "    transformed_column = pow(df[x], 3) + exp(df[y])\n",
    "    df_transformed = df.withColumn(f\"transformed_value_{x}_{y}\", transformed_column)\n",
    "    return df_transformed\n",
    "\n",
    "def unique(df, column):\n",
    "    unique_count = df.agg(countDistinct(col(f\"{column}\")).alias(f\"unique_{column}_count\"))\n",
    "    return unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transformed_column(df, \"cont_4\", \"cont_9\")\n",
    "df_unique = unique(df, \"cat_8\")\n",
    "\n",
    "df_transformed.show(10)\n",
    "df_unique.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580cc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d153db66",
   "metadata": {},
   "source": [
    "#### Si quisieramos conectarnos directo con Kinesis Data Stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Step 1: Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"StreamingInference\").getOrCreate()\n",
    "\n",
    "# Step 2: Read the data in real-time\n",
    "df = spark.readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"kinesis_stream_name\") \\\n",
    "    .option(\"awsAccessKey\", \"your_aws_access_key\") \\\n",
    "    .option(\"awsSecretKey\", \"your_aws_secret_key\") \\\n",
    "    .option(\"region\", \"us-west-2\") \\\n",
    "    .load()\n",
    "\n",
    "# Step 3: Transform the data\n",
    "df_transformed = df.selectExpr(\"cast (data as string) as data\") \\\n",
    "    .groupBy(window(df.timestamp, \"10 minutes\", \"5 minutes\")) \\\n",
    "    .agg(count(\"data\").alias(\"count\"))\n",
    "\n",
    "# Step 4: Store the results\n",
    "query = df_transformed.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/streaming_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/streaming_checkpoints\") \\\n",
    "    .start()\n",
    "\n",
    "# Step 5: Start the streaming process\n",
    "query.awaitTermination()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5740a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72f71c0c",
   "metadata": {},
   "source": [
    "#### Paso 4: \n",
    "\n",
    "###### check model_pipeline.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
